{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Open Street Map into MongoDB\n",
    "#### Map Area\n",
    "Bellingham, WA, USA (greater area for minimum size requirements)\n",
    "\n",
    "Extract pulled from openstreetmap.org using <a href=\"https://www.openstreetmap.org/export\">Overpass API</a>.\n",
    "```\n",
    "<bounds minlat=\"48.6020000\" minlon=\"-122.8244000\" maxlat=\"49.0027000\" maxlon=\"-122.0787000\"/>\n",
    "```\n",
    "<a id=\"contents\"></a>\n",
    "## Contents\n",
    "<ul>\n",
    "    <li><a href=\"#initStats\">Document Structure and Initial Stats</a></li>\n",
    "    <li><a href=\"#tagAudits\">Sample Tag Audits</a></li>\n",
    "    <li><a href=\"#trouble\">Trouble in the Data</a></li>\n",
    "    <li><a href=\"#load\">Cleanup, Write to JSON, Load into MongoDB</a></li>\n",
    "    <li><a href=\"#mongoStats\">Import and MongoDB Stats</a></li>\n",
    "    <li><a href=\"#mongoAudit\">Auditing in Mongo</a></li>\n",
    "    <li><a href=\"#finalQ\">Final Queries: using lookup; another audit; indexes</a></li>\n",
    "    <li><a href=\"#recommendation\">A Parting Recommendation</a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"initStats\"></a>\n",
    "## Document Structure and Initial Stats\n",
    "First, we'll look at the expected document and database structures before and after. After that, we can dive into the tags, and make a cleanup plan for some of them.\n",
    "\n",
    "To build our JSON document, we skip the outer tags (\"osm\", \"note\", \"meta\", and \"bounds\") and focus on the three main elements (\"node\", \"way\", and \"relation\") for top-level documents, loading them into a single MongoDB collection. Their attributes and subelements (\"tag\", \"nd\", and \"member\") are their subdocuments.\n",
    "\n",
    "Element creation attributes (\"changeset\", \"timestamp\", \"uid\", \"user\", and \"version\") become keys in one subdocument called \"created\". Node lattitude and longitude form a single positional vector array under the \"pos\" key. The \"ref\" attribute of the \"nd\" subelements similarly form a list under the \"node_refs\" key of way documents. Relation elements' \"member\" subelements go into a list of subdocuments with their attribute keys (\"re\", \"role\", \"type\").\n",
    "\n",
    "While tag elements only have \"k\" and \"v\" attributes, they have the messiest range of possible values that pose structural challenges as well as copy editing challenges. This is where we focus our cleanup efforts. Unlike other elements, we drop tag attribute keys, and use their values as keys and values in the JSON document.\n",
    "\n",
    "Okay, let's get a bird's eye view of the document. Since we're cleaning up before loading into MongoDB, we'll just make some quick and dirty pandas dataframes for this initial analyis. One thing worth noting is that nodes are the most common top-level element, which the nd element is the most common element, which means some nodes get referenced more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import numpy as np\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "# My modules.\n",
    "import osm_structure_audit\n",
    "import clean_and_write\n",
    "import mongo_audit\n",
    "\n",
    "filename = \"greater_bellingham.osm\"\n",
    "\n",
    "list_query = lambda cursor: [doc for doc in cursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sub_els</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>element_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>osm</th>\n",
       "      <td>1</td>\n",
       "      <td>{way, note, meta, tag, node, bounds, relation}</td>\n",
       "      <td>{version, generator}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>note</th>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta</th>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{osm_base}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bounds</th>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>{maxlat, minlat, minlon, maxlon}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node</th>\n",
       "      <td>355044</td>\n",
       "      <td>{tag}</td>\n",
       "      <td>{version, timestamp, uid, lat, id, lon, change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <td>131881</td>\n",
       "      <td>{}</td>\n",
       "      <td>{v, k}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>30179</td>\n",
       "      <td>{tag, nd}</td>\n",
       "      <td>{version, timestamp, uid, id, changeset, user}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nd</th>\n",
       "      <td>405590</td>\n",
       "      <td>{}</td>\n",
       "      <td>{ref}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relation</th>\n",
       "      <td>554</td>\n",
       "      <td>{member, tag}</td>\n",
       "      <td>{version, timestamp, uid, id, changeset, user}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member</th>\n",
       "      <td>18179</td>\n",
       "      <td>{}</td>\n",
       "      <td>{type, role, ref}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count                                         sub_els  \\\n",
       "element_type                                                           \n",
       "osm                1  {way, note, meta, tag, node, bounds, relation}   \n",
       "note               1                                              {}   \n",
       "meta               1                                              {}   \n",
       "bounds             1                                              {}   \n",
       "node          355044                                           {tag}   \n",
       "tag           131881                                              {}   \n",
       "way            30179                                       {tag, nd}   \n",
       "nd            405590                                              {}   \n",
       "relation         554                                   {member, tag}   \n",
       "member         18179                                              {}   \n",
       "\n",
       "                                                     attributes  \n",
       "element_type                                                     \n",
       "osm                                        {version, generator}  \n",
       "note                                                         {}  \n",
       "meta                                                 {osm_base}  \n",
       "bounds                         {maxlat, minlat, minlon, maxlon}  \n",
       "node          {version, timestamp, uid, lat, id, lon, change...  \n",
       "tag                                                      {v, k}  \n",
       "way              {version, timestamp, uid, id, changeset, user}  \n",
       "nd                                                        {ref}  \n",
       "relation         {version, timestamp, uid, id, changeset, user}  \n",
       "member                                        {type, role, ref}  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of node attributes get cut off in the output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'changeset', 'id', 'lat', 'lon', 'timestamp', 'uid', 'user', 'version'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_df, tag_df = osm_structure_audit.get_eldf_tagdf(filename)\n",
    "\n",
    "# iter() stops parsing after 105 subelements on my computer.\n",
    "# Consequently, it won't get to way and relation elements within osm.\n",
    "# I'll add them here.\n",
    "el_df.loc[\"osm\", \"sub_els\"].add(\"way\")\n",
    "el_df.loc[\"osm\", \"sub_els\"].add(\"relation\")\n",
    "el_df\n",
    "print(\"The set of node attributes get cut off in the output:\")\n",
    "el_df.loc[\"node\", \"attributes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags\n",
    "It's also worth noting that every top-level element uses tag subelements. Drilling down into the tags, where most of the cleaning needs to happen, we want to find as many problems as we can before loading the data into MongoDB. That way we can clean it as we write it to the JSON file.\n",
    "\n",
    "If we were doing a comprehensive cleanup, we might divide the work among a team. We might want to get an idea of which tags will be quick wins and which will require more intensive investigation and cleanup. We could do that by getting a set of unique values for each key and getting a count of those unique values. There are hundreds of tag keys, some only used once or twice, and others used thousands of times. (See below.)\n",
    "\n",
    "I'm neither doing a comprehensive cleanup, nor am I working on a team on this school project. I only need to clean a couple of tag types for this project. That said, I wanted to have fun and solve a few extra problems, so organizing the data into manageable, targetable chunks is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag_use_count</th>\n",
       "      <th>val_set</th>\n",
       "      <th>uniq_count</th>\n",
       "      <th>usage_per_uniq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>created_by</th>\n",
       "      <td>1836</td>\n",
       "      <td>{Potlatch alpha, Potlatch 0.10b, polyshp2osm-m...</td>\n",
       "      <td>5</td>\n",
       "      <td>367.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highway</th>\n",
       "      <td>20660</td>\n",
       "      <td>{street_lamp, secondary, road, raceway, cyclew...</td>\n",
       "      <td>39</td>\n",
       "      <td>529.743590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>7414</td>\n",
       "      <td>{http://www.waymarking.com/waymarks/WMRJGA_Neu...</td>\n",
       "      <td>176</td>\n",
       "      <td>42.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>11152</td>\n",
       "      <td>{Tweedle Dee Tweedle Dum, Woodfern Way, Brandy...</td>\n",
       "      <td>6951</td>\n",
       "      <td>1.604373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref</th>\n",
       "      <td>859</td>\n",
       "      <td>{E, 11G, 87, 7G, MON 26, 190, MON 43, MON 5E-9...</td>\n",
       "      <td>204</td>\n",
       "      <td>4.210784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complete</th>\n",
       "      <td>1</td>\n",
       "      <td>{yes}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distance</th>\n",
       "      <td>1</td>\n",
       "      <td>{1200 miles}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>1</td>\n",
       "      <td>{no}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>osmc:symbol</th>\n",
       "      <td>1</td>\n",
       "      <td>{yellow:white_frame:yellow_triangle}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backcountry</th>\n",
       "      <td>1</td>\n",
       "      <td>{yes}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1032 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag_use_count                                            val_set  \\\n",
       "tag_key                                                                         \n",
       "created_by            1836  {Potlatch alpha, Potlatch 0.10b, polyshp2osm-m...   \n",
       "highway              20660  {street_lamp, secondary, road, raceway, cyclew...   \n",
       "source                7414  {http://www.waymarking.com/waymarks/WMRJGA_Neu...   \n",
       "name                 11152  {Tweedle Dee Tweedle Dum, Woodfern Way, Brandy...   \n",
       "ref                    859  {E, 11G, 87, 7G, MON 26, 190, MON 43, MON 5E-9...   \n",
       "...                    ...                                                ...   \n",
       "complete                 1                                              {yes}   \n",
       "distance                 1                                       {1200 miles}   \n",
       "education                1                                               {no}   \n",
       "osmc:symbol              1               {yellow:white_frame:yellow_triangle}   \n",
       "backcountry              1                                              {yes}   \n",
       "\n",
       "             uniq_count  usage_per_uniq  \n",
       "tag_key                                  \n",
       "created_by            5      367.200000  \n",
       "highway              39      529.743590  \n",
       "source              176       42.125000  \n",
       "name               6951        1.604373  \n",
       "ref                 204        4.210784  \n",
       "...                 ...             ...  \n",
       "complete              1        1.000000  \n",
       "distance              1        1.000000  \n",
       "education             1        1.000000  \n",
       "osmc:symbol           1        1.000000  \n",
       "backcountry           1        1.000000  \n",
       "\n",
       "[1032 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In cases in which the dataset is too large to read the value sets into memory, the script could be rewritten to only get the element and tag lists and counts. That is, create the same dataframe without the value set column. Since you'd need to parse the XML to count each set, this would come at a considerable compute time cost. At that point, you might as well load it into a temporary database.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML tag stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag_use_count</th>\n",
       "      <th>uniq_count</th>\n",
       "      <th>usage_per_uniq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1032.000000</td>\n",
       "      <td>1032.000000</td>\n",
       "      <td>1032.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>127.791667</td>\n",
       "      <td>18.108527</td>\n",
       "      <td>14.431499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>885.506474</td>\n",
       "      <td>231.232681</td>\n",
       "      <td>73.255911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.207368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>20.800000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>107.900000</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>19.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20660.000000</td>\n",
       "      <td>6951.000000</td>\n",
       "      <td>1314.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag_use_count   uniq_count  usage_per_uniq\n",
       "count    1032.000000  1032.000000     1032.000000\n",
       "mean      127.791667    18.108527       14.431499\n",
       "std       885.506474   231.232681       73.255911\n",
       "min         1.000000     1.000000        1.000000\n",
       "10%         1.000000     1.000000        1.000000\n",
       "20%         1.000000     1.000000        1.000000\n",
       "30%         2.000000     1.000000        1.000000\n",
       "40%         2.000000     1.000000        1.000000\n",
       "50%         3.000000     2.000000        1.000000\n",
       "60%         3.000000     2.000000        1.500000\n",
       "70%         7.000000     3.000000        2.207368\n",
       "80%        20.800000     4.000000        5.000000\n",
       "90%       107.900000     8.900000       19.315789\n",
       "max     20660.000000  6951.000000     1314.750000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_df.describe(percentiles=np.arange(start=.1, stop=1, step=.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,032 unique tag keys. Most are used three or fewer times, and one is used 20,660 times, the most of any. Almost half of the tags only have a single unique value. The \"name\" key has the most unique values, 6,951, which makes it a good candidate for an index.\n",
    "\n",
    "About 20% have two unique values. These are good candidates for boolean values, as are tags with a single unique value. However, many OSM tags with \"yes\" and \"no\" as possible values also include other possible values. We still need to take them each on a case-by-case basis, consulting the OSM wiki. I did this in some cases, but not all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents.</a>\n",
    "<a id=\"tagAudits\"></a>\n",
    "## Sample Tag Audits\n",
    "### Phone audit\n",
    "Say we're tasked with formatting all \"phone\" tags. First, we'll want to find what needs to be fixed, then we'll need to fix it.\n",
    "\n",
    "We can start by laying eyes on the list. Uncomment and run the below code to see the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to print full list.\n",
    "# tag_df.loc['phone', 'val_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling through the list, we see that the biggest issue is that they are formatted in different ways using different separating characters, including or not including the country code, with some having extensions.\n",
    "\n",
    "Additionally, at least one of these entries is two, semicolon-separated phone numbers. We'll need to audit listed values, too. But, I handle making this field a list elsewhere, so we'll just focus on formatting one phone number at a time here.\n",
    "\n",
    "We want to check the area codes and length at least. A more rigorous audit might include finding all valid prefixes for each area code and validating the prefixes as well.\n",
    "\n",
    "We can ignore the extensions for now and strip them off for the audit.\n",
    "\n",
    "We'll start our script with that, then pull all of the non-digit characters out, make sure there are at least 10 digits long, and check the first four digits to make sure they contain the right area code and possibly country code. It doesn't have to have a country code. We'll take care of that in the cleanup script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The assumption here is that there are no foreign numbers with a country code and area code that matches our local pattern (e.g. +13 60...), but I didn't see an instance of that, and it would be surprising to find an instance in this extract. A bigger extract might require a more robust audit.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3067562314'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18882293770'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'12537097453'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'16045579901'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'16048567472'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['18004633339', '8004633339']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'16045411217'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18559173767'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18884933189'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18662260465'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18444627342'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18664558489'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'18663830777'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'13063988300'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'12064293813'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_four_re = re.compile(r'1*360')\n",
    "\n",
    "def ex_audit_phone(num):\n",
    "    # Could check other separators, but we know this is all we have.\n",
    "    is_list = \";\" in num\n",
    "    if not is_list:\n",
    "        # Strip extensions.\n",
    "        # Not a clean strip, but removes numbers, which is all we care about.\n",
    "        x_idx = num.find(\"x\")\n",
    "        if x_idx > -1:\n",
    "            num = num[:(x_idx)]\n",
    "        # Remove all non-digits.\n",
    "        num = re.sub(r'\\D', \"\", num)\n",
    "        if len(num) >= 10 and first_four_re.match(num[:4]):\n",
    "            num = None\n",
    "    else:\n",
    "        num_lst = num.split(\";\")\n",
    "        num = list()\n",
    "        for num_it in num_lst:\n",
    "            num.append(ex_audit_phone(num_it))\n",
    "            \n",
    "    return num\n",
    "\n",
    "for num in tag_df.loc['phone', 'val_set']:\n",
    "    ex_audit_phone(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengths all look good. The only real problem with these is that they don't conform to the expected area code. The toll-free numbers are fine, as are the \"253\", \"206\", and \"604\" area codes, which are from a neighboring regions with the same country code. For our purposes, I'm going to say that \"306\" is a typo, not a Saskatchewan number, which is quite far away. They both have valid prefixes for Whatcom County (where Bellingham is) and for Saskatchewan, but you could try to verify with a phone call or lookup if you really needed to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone cleanup\n",
    "The <a href='https://wiki.openstreetmap.org/wiki/Key:phone'>OSM wiki for the phone tag key</a> sets two standard formats:\n",
    "```\n",
    "phone=+<country code> <area code> <local number>\n",
    "phone=+<country code>-<area code>-<local number>\n",
    "```\n",
    "We'll use the second format. We want to keep extensions, so we'll tack those on at the end:\n",
    "```\n",
    "phone=+<country code>-<area code>-<local number> x<extension>\n",
    "```\n",
    "We'll create a regular expression for the target format, find all numbers that don't match, remove all non-numerical characters, make sure there's a \"+1\" at the beginning, look for an extension, and insert separators where they belong.\n",
    "\n",
    "We can use a quick and dirty regular expression for this extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+1-360-555-9999 x1234'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'+1-360-398-8300'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PHONE_RE = re.compile(r'\\+1-\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d')\n",
    "WRONG_AC_RE = re.compile(r'1*306')\n",
    "\n",
    "number_lst = [\"(360) 555-9999 ext. 1234\", \"+1 306-398-8300\"]\n",
    "\n",
    "def ex_format_phone(num):\n",
    "    if not PHONE_RE.fullmatch(num):\n",
    "        num = re.sub(r'\\D', \"\", num)\n",
    "        if WRONG_AC_RE.match(num):\n",
    "            num = re.sub(\"306\", \"360\", num, count=1)\n",
    "        if num[0] != \"1\":\n",
    "            num = \"1\" + num\n",
    "        if len(num) > 12:\n",
    "            num = num[:11] + \" x\" + num[11:]\n",
    "        num = \"+\" + num[0] + \"-\" + num[1:4] + \"-\" + num[4:7] + \"-\" + num[7:]\n",
    "        \n",
    "    return num\n",
    "\n",
    "for num in number_lst:\n",
    "    ex_format_phone(num)\n",
    "\n",
    "# for num in tag_df.loc['phone', 'val_set']:\n",
    "#     ex_format_phone(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street audit\n",
    "Now, say we're auditing street names in the \"addr:street\" tag. In addition to taking care of simple things like capitalization, part of the cleanup is to enforce a standard for street types. For instance, we want all avenues to be called \"Avenue\", not \"Ave.\" or \"Ave\", or anything else.\n",
    "\n",
    "We'll search for exceptions to acceptable street type values. Then, we can map those exceptions to the formatted string when we're running our cleanup and writing to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#101',\n",
       " '#215',\n",
       " '13',\n",
       " 'Ave',\n",
       " 'Ave.',\n",
       " 'Bakerview',\n",
       " 'Blvd',\n",
       " 'Broadway',\n",
       " 'Count',\n",
       " 'Dr',\n",
       " 'Forest',\n",
       " 'Hwy',\n",
       " 'Meridian',\n",
       " 'North',\n",
       " 'Pkwy',\n",
       " 'Rd',\n",
       " 'Rd.',\n",
       " 'Road3',\n",
       " 'St',\n",
       " 'St.',\n",
       " 'Street\\\\',\n",
       " 'WA-542',\n",
       " 'street'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # regex to grab the street name from the end of the string.\n",
    "# street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# Acceptable street type formats.\n",
    "street_type_lst = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\",\n",
    "                   \"Square\", \"Lane\", \"Road\", \"Trail\", \"Parkway\", \"Highway\",\n",
    "                   \"Way\"]\n",
    "# Exceptions, street types not matching format.\n",
    "street_type_exceptions_set = set()\n",
    "\n",
    "for street in tag_df.loc[\"addr:street\", \"val_set\"]:\n",
    "#     # If using regex.\n",
    "#     match = street_type_re.search(street)\n",
    "#     if match:\n",
    "#         street_type = match.group()\n",
    "#         if street_type not in street_type_lst:\n",
    "#             street_type_exceptions_set.add(street_type)\n",
    "\n",
    "    # If not using regex, just split and grab.\n",
    "    street_type = street.split()[-1]\n",
    "    if street_type not in street_type_lst:\n",
    "        street_type_exceptions_set.add(street_type)\n",
    "        \n",
    "street_type_exceptions_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are street names missing suffixes. Some are abbreviations or typos, or something else. These can be easily added to the mapping dictionary below.\n",
    "\n",
    "Broadway is actually a street in Bellingham without a suffix, so we can ignore it. We can ignore \"WA-542\" which is a highway.\n",
    "\n",
    "Forest could be Forest Street or Forest Court. Bakerview could be Bakerview Road or Bakerview Spur. Meridian could be Meridian Street or Guide Meridian Road. We need to manually check those.\n",
    "\n",
    "The numbers are probably unit numbers, but I want to make sure they aren't highway numbers or something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'addr:street', 'v': 'North Forest'}\n",
      "{'k': 'addr:street', 'v': 'Ellis St. #215'}\n",
      "{'k': 'addr:street', 'v': 'West Bakerview'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Old Highway 99 North'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'East Holly Street, #101'}\n",
      "{'k': 'addr:street', 'v': 'Highway 13'}\n",
      "{'k': 'addr:street', 'v': 'Highway 13'}\n",
      "{'k': 'addr:street', 'v': 'Chuckanut Drive North'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Guide Meridian'}\n",
      "{'k': 'addr:street', 'v': 'Chuckanut Drive North'}\n",
      "{'k': 'addr:street', 'v': 'Birch Bay Lynden Road3'}\n",
      "{'k': 'addr:street', 'v': 'Meadowbrook Count'}\n"
     ]
    }
   ],
   "source": [
    "unhandled_lst = [\"13\", \"#101\", \"#215\", \"Bakerview\", \"Count\", \"Forest\",\n",
    "                 \"Meridian\", \"North\", \"Road3\"]\n",
    "unhandled_els = list()\n",
    "for _, el in ET.iterparse(source=filename, events=('start',)):\n",
    "    if el.tag == \"tag\" and el.attrib[\"k\"] == \"addr:street\" \\\n",
    "        and el.attrib[\"v\"].split()[-1] in unhandled_lst:\n",
    "            unhandled_els.append(el)\n",
    "for el in unhandled_els:\n",
    "    print(el.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no North Forest Court, so we know this is North Forest Street. Without the prefix, we would have had to find the node or way on OSM to be sure. Same with West Bakerview Road.\n",
    "\n",
    "Colloquially, Guide Meridian Road is called Guide Meridian (or \"the Guide\" if you're a true local). Guide Meridian more properly refers to a district on the Guide. In the interest of being clear and consistent with other map sources (i.e. Google Maps), we'll add \"Road\".\n",
    "\n",
    "Old Highway 99 North and Chuckanut Drive North are proper and can be ignored. Highway 13 can be ignored, but in another extract 13 might be an apartment number.\n",
    "\n",
    "\"Count\" is a typo of \"Court\". \"Road3\" is a typo.\n",
    "\n",
    "The numbers look like apartment or suite numbers. We can send those to ```\"addr\": {\"unit\": <unit number>}``` elsewhere in the code. But, we'll have to be careful not treat Highway 13 or WA-542 as unit numbers.\n",
    "### Street cleanup\n",
    "This cleanup is specific to this extract. A future extract of the same area will likely contain errors not handled by this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('North Forest Street', None),\n",
       " ('Ellis Street', '#215'),\n",
       " ('West Bakerview Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Old Highway 99 North', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('East Holly Street', '#101'),\n",
       " ('Highway 13', None),\n",
       " ('Highway 13', None),\n",
       " ('Chuckanut Drive North', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Guide Meridian Road', None),\n",
       " ('Chuckanut Drive North', None),\n",
       " ('Birch Bay Lynden Road', None),\n",
       " ('Meadowbrook Court', None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exceptions mapped to acceptable street type formats.\n",
    "ex_STREET_TYPE_MAP = {\"Ave\": \"Avenue\", \"Ave.\": \"Avenue\", \"Blvd\": \"Boulevard\",\n",
    "    \"Bakerview\": \"Bakerview Road\", \"Count\": \"Court\", \"Dr\": \"Drive\",\n",
    "    \"Forest\": \"Forest Street\", \"Hwy\": \"Highway\", \"Meridian\": \"Meridian Road\",\n",
    "    \"Pkwy\": \"Parkway\", \"Rd\": \"Road\", \"Rd.\": \"Road\", \"Road3\": \"Road\",\n",
    "    \"St\": \"Street\", \"St.\": \"Street\", \"Street,\": \"Street\", \"Street\\\\\": \"Street\",\n",
    "    \"street\": \"Street\"}\n",
    "\n",
    "def ex_clean_street(street):\n",
    "    unit = None\n",
    "    street_type = street.split()[-1]\n",
    "    if \"#\" in street_type:\n",
    "        unit = street_type\n",
    "        street = \" \".join(street.split()[:-1])\n",
    "        street_type = street.split()[-1]\n",
    "    if street_type in ex_STREET_TYPE_MAP.keys():\n",
    "         street = \" \".join(street.split()[:-1]) + \\\n",
    "            \" \" + ex_STREET_TYPE_MAP[street_type]\n",
    "            \n",
    "    return street, unit\n",
    "\n",
    "handled_lst = list()\n",
    "for el in unhandled_els:\n",
    "    handled_lst.append(ex_clean_street(el.attrib[\"v\"]))\n",
    "handled_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further auditing\n",
    "Those are my two examples of showing my work as I audit the data, which is all that's required for this assignment, but I'm choosing to do more. I'll highlight some findings, but I won't show all of my work. See the imported scripts.\n",
    "\n",
    "I'm not doing a comprehensive cleanup, but I decided to look at each and every tag because I want to familiarize myself with OSM, and with my dataset. I should note that, in the interest of time, I'm foregoing a rigorous audit from here, having satisfied the requirements of the assignment, so I will likely make some mistakes that I wouldn't otherwise in a full audit.\n",
    "\n",
    "I printed out all of the tag keys with their value sets and looked them over. I skimmed over tags that seemed pretty self-evident and didn't have any troublesome values that caught my eye. I checked the OSM wiki for the standards on the tags that did catch my eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's dig in!\n",
    "# # Uncomment the following block to print out all of the tag keys with their\n",
    "# # value sets.\n",
    "\n",
    "# for tag_key, tag_val_set in sorted(tag_df['val_set'].items()):\n",
    "#     (tag_key, tag_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents.</a>\n",
    "<a id=\"trouble\"></a>\n",
    "## Trouble in the Data\n",
    "### Structural trouble\n",
    "A few things quickly stand out. Some of them have to do with restructuring XML into JSON.\n",
    "\n",
    "#### Colon-separated keys\n",
    "\n",
    "For instance, many keys have colon-separated values that represent a hierarchical structure (e.g. \"addr:street=\" and \"addr:state=\"). These keys can be subdivided into lower-order documents. But, not all keys like this can be subdivided so easily. Some of the parent prefixes are also keys that stand alone and have their own values (e.g. \"turn=\", \"turn:lane=\"). We would need to create a child key identical to the parent key. This might have undesirable effects. That said, this might be a moot point if we were to group tags by parent elements. Since I have no analytical or use goals for this database, I chose to only subdivide a select list of keys in which the parents never stand alone: addr, cost, fire_hydrant, fuel, payment.\n",
    "\n",
    "#### Lists\n",
    "\n",
    "We also need to decide which tags have list values and which have scalar values. A starting point might be to identify those with semicolon-separated values by searching for values with semicolons. If you were to programmatically search for semicolons, you'd still need to look manually in those values that might employ a semicolon as natural language rather than as a separator, such as in \"note\" and \"description\" tags. Plus, what about those tags that warrant a list but happen to only have a scalar value in this extract? Rather than developing (or finding) an AI for this task, I decided to put eyes on each tag, consulting the wiki as necessary, and make a list by hand. That said, I did run a search for semicolons to catch some tags that I might have missed. Again, I'm only doing this part casually to create some problems to solve, beyond what's required.\n",
    "\n",
    "Other candidates for creating list values are those tags with keys that end in an underscore and a number, indicating multiple values for the same type of tag (e.g. 'name_1'). I compiled this list programmatically.\n",
    "\n",
    "The \"is_in\" key and all of its children is another quick check. We could omit it since this extract is exlcusive to Bellingham, WA. But, hypothetically, this data might be joined to other extracts, so I left it in and mapped it to standard value strings.\n",
    "\n",
    "Any key with a \"conditional\" suffix has a list.\n",
    "\n",
    "#### Other\n",
    "    \n",
    "The \"contact\" prefix is deprecated, so I removed it. These keys should hold list values.\n",
    "    \n",
    "Finally, I substituted some miscellaneous keys for others, due to deprecation, misuse, redundancy, etc. However, I'm choosing to leave the \"tiger\", \"gnis\", and \"nist\" tags alone in lieu of forming a broader plan for them. I did fix a couple of gnis typos.\n",
    "\n",
    "I do recommend replacing gnis:ST_num: with gnis:state_id and gnis:County_num: with gnis:county_id because they are more widely used, but I left them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouble with values\n",
    "The rest of the trouble is basically copy editing values. For instance, addresses, phone numbers, dates, etc. need to be formatted.\n",
    "\n",
    "#### Geographical scope\n",
    "\n",
    "Some elements point to locations outside of the map, in nearby towns: Ferndale, Lawrence, Eliza Island, Sudden Valley, Nooksack River; \"addr:postcode\" 98248 is Ferndale, 98262 is Lummi Island, 98266 is Maple Falls, 98284 is Skagit County which is neighboring but may also be a typo for Ferndale's zip. These might be legitimate inclusions. Wikipedia references with outside locations might be legitimate, for instance. They may point to the Bellingham page from the pages of nearby locations. I waited to investigate them once I'd loaded them into MongoDB.\n",
    "\n",
    "#### Data types\n",
    "\n",
    "I stored some values as numbers, but some of them have units tacked on or are otherwise messy. These mostly require a decision about whether to subdivide by unit, convert into a single unit, or leave as strings. I'm leaving them as strings to limit my scope of cleanup.\n",
    "\n",
    "#### Symbol and image links\n",
    "\n",
    "Multiple \"symbol\" tags link to an image at wikimedia.org. The \"symbol\" tag is for human-readable descriptions, not links. According to the OSM wiki, these links would be better placed in a \"wiki:symbol\" tag. There's some question about the proper way to link to images in the \"wiki:symbol\" tag. The OSM wiki entry on the tag states that these should only be filenames of files uploaded to the OSM wiki, not to external sites. No examples of this method exist in this extract, though. Most of the links do have identical filenames to identical symbols on the OSM wiki, so they could easily be stripped of their paths. However, one link does not and needs to be uploaded to OSM. I might do that once I gain that account privilege. In the meantime, a few forum users have stated, contrary to the OSM wiki article, that a link to wikimedia is fine. So, I just assigned the links to the appropriate key.\n",
    "\n",
    "#### The rest\n",
    "\n",
    "I found a long list of other copy edits to make, typos and formatting issues mostly. Some of which I handled. See the cleanup script for a commented list of what I left unhandled.\n",
    "\n",
    "I also created a laundry list of stickier issues that aren't simply edits to be made, and that I will not likely fix. They may require a FIXME tag in the OSM data, or an executive/team decision with regard for the intended use of the data.\n",
    "\n",
    "For instance, the OSM wiki doesn't include the \"wheelchair:entrance_width\" and \"wheelchair:step_height\" tags. They therefore have no default unit, and the tags in the extract don't give units. OSM defaults to metric units, but this is a U.S. dataset, so the contributor may have used imperial units. This warrants issuing a FIXME tag in the element, and perhaps creating a page on the wiki if no sufficient alternative tag exists. It also may fit nicely into the following category.\n",
    "\n",
    "#### Field trips and curiosities\n",
    "\n",
    "I collected some cases of what I'm calling \"field trips and curiosities.\" They include unexpected values that beckon a closer look at the data, if not also an investigation \"in real life\" (e.g. \"width\": \"Cedar Jumps Green Line\", \"maxspeed\": \"127\", \"start_date\": \"0000\"). Some of them just need a phone call for business hours or a bike ride with a tape measure. Data wrangling sometimes requires boots on the ground to establish ground truth.\n",
    "\n",
    "Alright, let's get to cleaning up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents.</a>\n",
    "<a id=\"load\"></a>\n",
    "## Cleanup, Write to JSON, Load into MongoDB\n",
    "Again, we want to clean the data as we write it to a JSON file, rather than after it's already loaded to MongoDB. So, we need functions that handle each fix while parsing through the XML.\n",
    "\n",
    "Below, we reshape the XML into JSON-compatible Python dictionaries and lists, representing the three top-level OSM elements. Then we write those as documents to a JSON file and import the file into a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"bham.json\"):\n",
    "    !del bham.json\n",
    "\n",
    "clean_and_write.process_map(file_in=filename, fo_pre=\"bham\", pretty=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = MongoClient(\"localhost:27017\")\n",
    "osm_db = mongo_client.osm\n",
    "bham_col = osm_db.bham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-03T09:19:09.285-0700\tconnected to: mongodb://localhost/\n",
      "2021-04-03T09:19:12.286-0700\t[########................] osm.bham\t40.3MB/109MB (36.8%)\n",
      "2021-04-03T09:19:15.286-0700\t[##################......] osm.bham\t83.3MB/109MB (76.1%)\n",
      "2021-04-03T09:19:16.959-0700\t[########################] osm.bham\t109MB/109MB (100.0%)\n",
      "2021-04-03T09:19:16.959-0700\t385777 document(s) imported successfully. 0 document(s) failed to import.\n"
     ]
    }
   ],
   "source": [
    "mongo_client.drop_database(\"osm\")\n",
    "# !\"C:\\Program Files\\MongoDB\\Tools\\100\\bin\\mongoimport.exe\"\n",
    "!mongoimport -d osm -c bham --file bham.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents.</a>\n",
    "<a id=\"mongoStats\"></a>\n",
    "## Import and MongoDB Stats\n",
    "The XML document is almost 77 MB, and grew to a JSON file about 115 MB big. After import, the new database total size (storage plus indexes) is 1.6 MB. However, due to compression, its data size is larger, 88.5 MB.\n",
    "\n",
    "There are 355,044 nodes, 30179 ways, and 354 relations. And, there are 921 unique users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76832450"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"greater_bellingham.osm\").st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114778234"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(\"bham.json\").st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db': 'osm',\n",
       " 'collections': 1,\n",
       " 'views': 0,\n",
       " 'objects': 385777,\n",
       " 'avgObjSize': 229.51770064052548,\n",
       " 'dataSize': 88542650.0,\n",
       " 'storageSize': 4096.0,\n",
       " 'indexes': 1,\n",
       " 'indexSize': 1622016.0,\n",
       " 'totalSize': 1626112.0,\n",
       " 'scaleFactor': 1.0,\n",
       " 'fsUsedSize': 97910169600.0,\n",
       " 'fsTotalSize': 255465693184.0,\n",
       " 'ok': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_db.command(\"dbstats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys per document type:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_id': 'node', 'count': 355044},\n",
       " {'_id': 'way', 'count': 30179},\n",
       " {'_id': 'relation', 'count': 554}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-load OSM node count matches MongoDB count?\n",
      "True\n",
      "Pre-load OSM way count matches MongoDB count?\n",
      "True\n",
      "Pre-load OSM relation count matches MongoDB count?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Count all documents by document type.\n",
    "doc_count_lst = mongo_audit.check_doc_counts_by(coll=bham_col, count_k=\"_id\",\n",
    "                                doc_type_lst=[\"node\", \"way\", \"relation\"],\n",
    "                                group_k=\"doc_type\")\n",
    "print(\"Keys per document type:\")\n",
    "doc_count_lst\n",
    "for count in doc_count_lst:\n",
    "    print(\"Pre-load OSM\", count[\"_id\"], \"count matches MongoDB count?\")\n",
    "    print(count[\"count\"] == el_df.loc[count[\"_id\"], \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'unique_users': 921}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_query(mongo_audit.get_unique_users(bham_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents.</a>\n",
    "<a id=\"mongoAudit\"></a>\n",
    "## Auditing in Mongo\n",
    "### Updating values\n",
    "Something I noticed while working with the data were a lot of addresses lacking states. I decided to fix them now that I can query.\n",
    "\n",
    "I'm guessing every address should be in Washington State. This was an accurate statement when I used a map that I didn't draw by hand overlapping into British Columbia; we'll pretend it's true here.\n",
    "\n",
    "I don't want to corrupt the data, so I will only set the state for addresses with zip codes. If I wanted to attack this problem with a map that included BC and WA, I might get lists of BC and WA zip codes and map the new state value accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating states:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Address Key</th>\n",
       "      <th>Zip</th>\n",
       "      <th>State</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pre_update</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>1749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matched</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modified</th>\n",
       "      <td>0.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post_update</th>\n",
       "      <td>1103.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>1749.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Address Key     Zip   State  Address\n",
       "Count                               \n",
       "Pre_update   1103.0   689.0   1749.0\n",
       "Matched      1103.0     0.0      0.0\n",
       "Modified        0.0   546.0      0.0\n",
       "Post_update  1103.0  1234.0   1749.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo_audit.update_states(bham_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural checks\n",
    "Let's see if tag keys were subdivided properly. I'm curious about bicycle services specifically. So, I'll check the services tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'node', 'count': 13},\n",
       " {'_id': 'way', 'count': 3529},\n",
       " {'_id': 'relation', 'count': 1}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count service documents.\n",
    "mongo_audit.check_doc_counts_by(coll=bham_col, count_k=\"service\",\n",
    "                                   doc_type_lst=[\"node\", \"way\", \"relation\"],\n",
    "                                   group_k=\"doc_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with bicycle services, shops, and/or bike repair stations:\n",
      "\n",
      "[{'_id': '255801449', 'name': ['Fairhaven Bike and Ski'], 'shop': ['bicycle']},\n",
      " {'_id': '255801452',\n",
      "  'name': ['The Hub'],\n",
      "  'service': {'bicycle': {'diy': 'yes', 'second_hand': 'yes'}},\n",
      "  'shop': ['bicycle']},\n",
      " {'_id': '1976759477', 'name': ['Trek Cycles'], 'shop': ['bicycle']},\n",
      " {'_id': '3191111061', 'name': ['Fanatik Bike Co.'], 'shop': ['bicycle']},\n",
      " {'_id': '3497698417',\n",
      "  'amenity': ['bicycle_repair_station'],\n",
      "  'note': 'Western Washington University',\n",
      "  'opening_hours': ['24/7'],\n",
      "  'service': {'bicycle': {'chain_tool': 'no'}}},\n",
      " {'_id': '4049510936', 'amenity': ['bicycle_repair_station']},\n",
      " {'_id': '4049598206',\n",
      "  'amenity': ['bicycle_repair_station'],\n",
      "  'note': 'Wade King Rec Center',\n",
      "  'opening_hours': ['24/7'],\n",
      "  'service': {'bicycle': {'pump': 'yes'}}},\n",
      " {'_id': '4049598207', 'amenity': ['bicycle_repair_station']},\n",
      " {'_id': '4176487913',\n",
      "  'amenity': ['bicycle_repair_station'],\n",
      "  'note': 'Haggard Hall',\n",
      "  'opening_hours': ['24/7'],\n",
      "  'service': {'bicycle': {'pump': 'yes'}}},\n",
      " {'_id': '4710095395',\n",
      "  'amenity': ['cafe'],\n",
      "  'description': 'COFFEE + BEER + BICYCLE REPAIR',\n",
      "  'name': ['Cafe Velo'],\n",
      "  'opening_hours': ['Tu-Th 07:00-19:00', 'Sa, Su 10:00-17:00', 'Mo-Tu Off'],\n",
      "  'service': {'bicycle': {'cleaning': 'yes',\n",
      "                          'parts': 'yes',\n",
      "                          'pump': 'yes',\n",
      "                          'repair': 'yes'}},\n",
      "  'shop': ['bicycle']},\n",
      " {'_id': '5157798082',\n",
      "  'amenity': ['bicycle_repair_station'],\n",
      "  'note': 'Front Street and 4th Street',\n",
      "  'opening_hours': ['24/7'],\n",
      "  'service': {'bicycle': {'pump': 'yes', 'tools': 'yes'}}},\n",
      " {'_id': '5336395963',\n",
      "  'name': [\"Jack's Bicycle Center\"],\n",
      "  'opening_hours': ['Mo-Fr 09:30-18:00', 'Sa 09:30-17:00'],\n",
      "  'service': {'bicycle': {'rental': 'yes', 'repair': 'yes'}},\n",
      "  'shop': ['bicycle']},\n",
      " {'_id': '5671818759',\n",
      "  'amenity': ['bicycle_repair_station'],\n",
      "  'fee': 'no',\n",
      "  'opening_hours': ['24/7'],\n",
      "  'service': {'bicycle': {'pump': 'yes', 'tools': 'yes'}}},\n",
      " {'_id': '6478647118',\n",
      "  'name': ['Seattle Electric Bike Co.'],\n",
      "  'service': {'bicycle': {'dealer': 'yes', 'ebike': 'yes', 'retail': 'yes'}},\n",
      "  'shop': ['bicycle']}]\n"
     ]
    }
   ],
   "source": [
    "mongo_audit.get_bike_services(coll=bham_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the service key was properly subdivided, and we have several bicycle services recorded in town.\n",
    "\n",
    "It's surprising that there's a relation with a service key. I would expect service keys specifying the types of services a shop offers (e.g. \"service:bicycle...\" to belong to nodes, while service keys with a single value (e.g. \"service=parking_aisle\") to describe a way.\n",
    "\n",
    "But, I validated documents in the cleanup script before writing to JSON. All node documents include a position, and do not include node references nor members. All way documents include node references but neither a position nor members. All relation documents include members, no position, and no node references.\n",
    "\n",
    "So, as long as the documents are properly formed, it's okay for services to be expressed as relations. I did find the three unexpectedly classified services, and they were vehicle repair and lube shops, where it might make sense to document the shop area or flow of the driveway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents.</a>\n",
    "<a id=\"finalQ\"></a>\n",
    "## Final Queries: using lookup; another audit; indexes\n",
    "For my final query, I want to find which users contributed the most-referenced service documents. That is, which service documents are referenced most, and who created them?\n",
    "\n",
    "In the process of answering this question, I found a problematic error in the data. Some documents reference other documents but classify them as the wrong type of documents. This disrupted the NoSQL equivalent of referential integrity when unwinding and regrouping to answer the question, which posed a problem when trying to write a \"join\" collection.\n",
    "\n",
    "So, before answering my question, I need to make sure all references to other documents agree on the document type.\n",
    "\n",
    "Once that is fixed, I create a \"join\" collection of referenced documents with a list of their referencing documents. This helps find the most-referenced service documents, and it would be generally useful for some hypothetical further queries.\n",
    "\n",
    "From there, I just sort by most-referenced, then look up which documents are service documents and grab their users.\n",
    "\n",
    "### Auditing references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ways point to the following types:\n",
      "[{'type': ['node']}]\n",
      "Relations point to the following types, and refer to them as:\n",
      "[{'referred_as': ['way', 'node'], 'type': ['node']},\n",
      " {'referred_as': ['relation'], 'type': ['relation']},\n",
      " {'referred_as': ['way'], 'type': ['way']}]\n"
     ]
    }
   ],
   "source": [
    "mongo_audit.audit_ref_types(coll=bham_col, coll_str=\"bham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ways pointing to non_nodes:\n",
      "\n",
      "\n",
      "Relations with mismatched referenced document types:\n",
      "[{'_id': '2317217',\n",
      "  'members': {'ref': '37125674', 'role': 'forward', 'type': 'node'},\n",
      "  'refs': {'_id': '37125674', 'doc_type': 'node'}},\n",
      " {'_id': '2859142',\n",
      "  'members': {'ref': '37125674', 'role': '', 'type': 'node'},\n",
      "  'refs': {'_id': '37125674', 'doc_type': 'node'}}]\n"
     ]
    }
   ],
   "source": [
    "mismatched_members_lst = mongo_audit.\\\n",
    "                            get_doctype_mismatches(coll=bham_col,\n",
    "                                                   coll_str=\"bham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Member to update:\n",
      "{'_id': '2317217', 'members': [{'type': 'way', 'ref': '37125674', 'role': 'forward'}]}\n",
      "Updated member:\n",
      "{'_id': '2317217', 'members': [{'type': 'node', 'ref': '37125674', 'role': 'forward'}]}\n",
      "Referenced document:\n",
      "{'_id': '37125674', 'doc_type': 'node'} \n",
      "\n",
      "Member to update:\n",
      "{'_id': '2859142', 'members': [{'type': 'way', 'ref': '37125674', 'role': ''}]}\n",
      "Updated member:\n",
      "{'_id': '2859142', 'members': [{'type': 'node', 'ref': '37125674', 'role': ''}]}\n",
      "Referenced document:\n",
      "{'_id': '37125674', 'doc_type': 'node'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_audit.\\\n",
    " fix_mismatched_refs(coll=bham_col,\n",
    "                     mismatched_members_lst=mismatched_members_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating reference table and using it\n",
    "Okay, now we can create our reference lookup table and use it to find out who contributed the most-referenced documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ref_docs. id is referenced doc; refers are referencing docs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_id': '7311005915', 'refers': ['782885150']}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_docs_col = mongo_audit.write_ref_docs(db=osm_db, coll=bham_col)\n",
    "\n",
    "print(\"Sample ref_docs. id is referenced doc; refers are referencing docs:\")\n",
    "list_query(ref_docs_col.find().limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '495336835',\n",
       "  'refer_count': 6,\n",
       "  'contributor': ['DunbarLoop'],\n",
       "  'contributer_uid': ['2032374']},\n",
       " {'_id': '273637364',\n",
       "  'refer_count': 6,\n",
       "  'contributor': ['DunbarLoop'],\n",
       "  'contributer_uid': ['2032374']},\n",
       " {'_id': '273637353',\n",
       "  'refer_count': 6,\n",
       "  'contributor': ['DunbarLoop'],\n",
       "  'contributer_uid': ['2032374']}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which service documents are referenced most, and who contributed them?\n",
    "list_query(mongo_audit.get_most_refd(coll=bham_col, field=\"service\", limit=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding indexes\n",
    "And, we might as well toss in a couple of indexes for hypothetical future use.\n",
    "\n",
    "We established earlier that name is a good candidate, since it has the highest cardinality. We'll make it a sparse index since not every document has a name.\n",
    "\n",
    "And, let's make a geospacial index on the coordinates field (\"pos\", should only be on nodes). We can't make it a unique index since contributors have added nodes with identical coordinates to other nodes. This could be an area for future cleanup, or it could be the result of a necessary hack (e.g. enabling identical coordinates to be tagged with multiple mutually exclusive tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name_-1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'pos_2d'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bham_col.create_index([(\"name\", pymongo.DESCENDING)], sparse=True)\n",
    "bham_col.create_index([(\"pos\", pymongo.GEO2D)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#contents\">Back to contents</a>\n",
    "<a id=\"recommendation\"></a>\n",
    "## A Parting Recommendation\n",
    "Disambiguate units of measurement and save as numbers to increase the utility of the database. This will enable calculations of all sorts, like calculating differences in elevation or finding the minimum maxheight on a route.\n",
    "\n",
    "Many users undoubtedly follow OSM standard and use metric units, but many contributors have used imperial units in measurements such as maxheight, as evidenced by the use of single and double quotes in the values indicating feet and inches. Those measurements are fairly easy to find and fix, but what of those measurements with no explicit unit? Do they follow the metric system or imperial?\n",
    "\n",
    "It would likely be necessary to reference outside sources, such as known bridge heights from the Department of Transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '538611921', 'maxheight': 0.0},\n",
       " {'_id': '665129111', 'maxheight': '13\"1\\''},\n",
       " {'_id': '665129114', 'maxheight': '13\"1\\''},\n",
       " {'_id': '665669883', 'maxheight': '9\\'10\"'},\n",
       " {'_id': '814346764', 'maxheight': 3.55}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_query(mongo_audit.get_by_field(coll=bham_col, field=\"maxheight\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
